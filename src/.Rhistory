}
language_name <- paste0('language_',reduce(dig, paste0),'_',reduce(multi, paste0))
language_expressions =  generate(digits, multiplicatives)
#for(i in 1:99){
# language_expressions[i] = c(language_expressions[i], language_name)
#}
language_expressions_df = as.data.frame(do.call(rbind, language_expressions))
colnames(language_expressions_df) <- c("morphology", "extension", "complexity")
language_expressions_df$language = language_name
language_expressions_df$word_n = length(c(dig, multi))
all_languages = rbind(all_languages, language_expressions_df)
# }
}
}
library(plyr)
library(dplyr)
library(ggplot2)
library(tidyr)
library(rje)
library(rlist)
library(stringr)
library(purrr)
#library(sets)
#library(combinat)
####################################
# Morphological piece - LoT dictionary
####################################
####################################
# Functions
####################################
# Useful version of as.numeric (as.numeric annoyingly can change values if applied to factors...)
as.numeric2 = function(x){
return(as.numeric(as.character(x)))
}
# Filter_within function: to ease further computations, of two expressions which result in the same meaning and are of the same complexity, we remove one
filter_within = function(set){
unwanted = c()
for(i in 1:length(set)){
for(j in 1:length(set)){
if(i==j){
next}
if(as.numeric2(unlist(set[i])[2]) == as.numeric2(unlist(set[j])[2])){
if((as.numeric2(unlist(set[j])[3]) == as.numeric2(unlist(set[i])[3])) & i < j){# if they are of equal complexity, throw out the one with larger index
unwanted = c(unwanted, j)
}
}
}
}
if(length(unwanted) >0){
set = set[-unwanted]}
return(set)
}
# Filter_across function: to ease further computations, of two expressions which result in the same meaning, we throw out the one with higher complexity
filter_across = function(set, lower_set){
unwanted = c()
for(i in 1:length(set)){
for(j in 1:length(lower_set)){
if(as.numeric2(unlist(set[i])[2]) == as.numeric2(unlist(lower_set[j])[2])){
unwanted = c(unwanted, i)}}}
if(length(unwanted) >0){
set = set[-unwanted]}
return(set)
}
## TOY PRIMITIVES
#primitives = list(1,2)
#digits = c()
#for(i in primitives){
# digits = c(digits, list(c(as.character(i), as.character(i), 1))) # list(LoT expression, extension, complexity), for primitives complexity = 1
#}
#primitives = list(5)
#multiplicatives = c()
#for(i in primitives){
# multiplicatives = c(multiplicatives, list(c(as.character(i), as.character(i), 1))) # list(LoT expression, extension, complexity), for primitives complexity = 1
#}
# Generate function for recursive application of higher_than, successor and +/- functions up to certain depth
#generate = function(depth, complexity_plus, complexity_minus, complexity_successor, complexity_multiplicatiton){
generate = function(digits, multiplicatives){
phrase1 = multiplicatives
number1 = c(digits, phrase1)
number1 = filter_within(number1)
start = 2
lowest_complexity = number1
#while (start < depth + 1){
while(!length(lowest_complexity)==99){
nam1 <- paste0("phrase", start)
assign(nam1, c())
nam2 <- paste0("number", start)
assign(nam2, c())
previous_number = eval(parse(text = paste0("number", start-1)))
#build phrase_start
for(first_element in previous_number){
for(second_element in phrase1){
current_multiplication = c(paste0(first_element[1], "_multiplication_", second_element[1]), as.numeric2(first_element[2]) * as.numeric2(second_element[2]), as.numeric2(first_element[3]) + as.numeric2(second_element[3]))
if(as.numeric2(current_multiplication[2])<100){#we do not want to include 100
assign(nam1, append(eval(parse(text = nam1)), list(current_multiplication)))
}
}
}
#append phrase_start to number_start
assign(nam2, append(eval(parse(text = nam2)), eval(parse(text = nam1))))
#build number_start from phrase_x and number_y combinations such that x+y = start
for(x in 1:start){ # +/- combinations
for(y in 1:start){
if(x+y == start){
for(first_element in eval(parse(text = paste0("phrase", x)))){
for(second_element in  eval(parse(text = paste0("number", y)))){ #format: morphology, extension, complexity
current_plus = c(paste0(first_element[1], "_plus_", second_element[1]), as.numeric2(first_element[2]) + as.numeric2(second_element[2]), as.numeric2(first_element[3]) + as.numeric2(second_element[3]))
current_minus = c(paste0(first_element[1], "_minus_", second_element[1]), as.numeric2(first_element[2]) - as.numeric2(second_element[2]), as.numeric2(first_element[3]) + as.numeric2(second_element[3]))
if(as.numeric2(current_plus[2])<100){
assign(nam2, append(eval(parse(text = nam2)), list(current_plus)))
}
if(as.numeric2(current_minus[2])> 0 & as.numeric2(current_minus[2])<100){
assign(nam2, append(eval(parse(text = nam2)), list(current_minus)))
}
}
}
}
}
}
assign(nam2, filter_within(eval(parse(text = nam2)))) #filter within
assign(nam2, filter_across(eval(parse(text = nam2)), lowest_complexity)) #filter across
lowest_complexity = append(lowest_complexity, eval(parse(text = nam2))) #append
start = start+1
}
# all = c()
#for(i in 1:depth){
# all = c(all, eval(parse(text = paste0("number", i))))
#}
#return(all)
return(lowest_complexity)
}
combinations = list(list(1), list(1,2), list(2))
digits_classes = combinations
multiplicative_classes = combinations
columns= c("morphology", "extension", "complexity", "language", "word_n")
all_languages = data.frame(matrix(nrow = 0, ncol = length(columns)))
colnames(all_languages) = columns
for(dig in digits_classes){
for(multi in multiplicative_classes){
# if(length(dig)+length(multi)<16){
digits = c()#put digits in good format
for(i in dig){
digits = c(digits, list(c(as.character(i), as.character(i), 1))) # list(LoT expression, extension, complexity), for primitives complexity = 1
}
multiplicatives = c()#put multiplicatives in good format
for(i in multi){
multiplicatives = c(multiplicatives, list(c(as.character(i), as.character(i), 1))) # list(LoT expression, extension, complexity), for primitives complexity = 1
}
language_name <- paste0('language_',reduce(dig, paste0),'_',reduce(multi, paste0))
language_expressions =  generate(digits, multiplicatives)
#for(i in 1:99){
# language_expressions[i] = c(language_expressions[i], language_name)
#}
language_expressions_df = as.data.frame(do.call(rbind, language_expressions))
colnames(language_expressions_df) <- c("morphology", "extension", "complexity")
language_expressions_df$language = language_name
language_expressions_df$word_n = length(c(dig, multi))
all_languages = rbind(all_languages, language_expressions_df)
# }
}
}
library(plyr)
library(dplyr)
library(ggplot2)
library(tidyr)
library(rje)
library(rlist)
library(stringr)
library(purrr)
#library(sets)
#library(combinat)
####################################
# Morphological piece - LoT dictionary
####################################
####################################
# Functions
####################################
# Useful version of as.numeric (as.numeric annoyingly can change values if applied to factors...)
as.numeric2 = function(x){
return(as.numeric(as.character(x)))
}
# Filter_within function: to ease further computations, of two expressions which result in the same meaning and are of the same complexity, we remove one
filter_within = function(set){
unwanted = c()
for(i in 1:length(set)){
for(j in 1:length(set)){
if(i==j){
next}
if(as.numeric2(unlist(set[i])[2]) == as.numeric2(unlist(set[j])[2])){
if((as.numeric2(unlist(set[j])[3]) == as.numeric2(unlist(set[i])[3])) & i < j){# if they are of equal complexity, throw out the one with larger index
unwanted = c(unwanted, j)
}
}
}
}
if(length(unwanted) >0){
set = set[-unwanted]}
return(set)
}
# Filter_across function: to ease further computations, of two expressions which result in the same meaning, we throw out the one with higher complexity
filter_across = function(set, lower_set){
unwanted = c()
for(i in 1:length(set)){
for(j in 1:length(lower_set)){
if(as.numeric2(unlist(set[i])[2]) == as.numeric2(unlist(lower_set[j])[2])){
unwanted = c(unwanted, i)}}}
if(length(unwanted) >0){
set = set[-unwanted]}
return(set)
}
## TOY PRIMITIVES
#primitives = list(1,2)
#digits = c()
#for(i in primitives){
# digits = c(digits, list(c(as.character(i), as.character(i), 1))) # list(LoT expression, extension, complexity), for primitives complexity = 1
#}
#primitives = list(5)
#multiplicatives = c()
#for(i in primitives){
# multiplicatives = c(multiplicatives, list(c(as.character(i), as.character(i), 1))) # list(LoT expression, extension, complexity), for primitives complexity = 1
#}
# Generate function for recursive application of higher_than, successor and +/- functions up to certain depth
#generate = function(depth, complexity_plus, complexity_minus, complexity_successor, complexity_multiplicatiton){
generate = function(digits, multiplicatives){
phrase1 = multiplicatives
number1 = c(digits, phrase1)
number1 = filter_within(number1)
start = 2
lowest_complexity = number1
#while (start < depth + 1){
while(!length(lowest_complexity)==99){
nam1 <- paste0("phrase", start)
assign(nam1, c())
nam2 <- paste0("number", start)
assign(nam2, c())
previous_number = eval(parse(text = paste0("number", start-1)))
#build phrase_start
for(first_element in previous_number){
for(second_element in phrase1){
current_multiplication = c(paste0(first_element[1], "_multiplication_", second_element[1]), as.numeric2(first_element[2]) * as.numeric2(second_element[2]), as.numeric2(first_element[3]) + as.numeric2(second_element[3]))
if(as.numeric2(current_multiplication[2])<100){#we do not want to include 100
assign(nam1, append(eval(parse(text = nam1)), list(current_multiplication)))
}
}
}
#append phrase_start to number_start
assign(nam2, append(eval(parse(text = nam2)), eval(parse(text = nam1))))
#build number_start from phrase_x and number_y combinations such that x+y = start
for(x in 1:start){ # +/- combinations
for(y in 1:start){
if(x+y == start){
for(first_element in eval(parse(text = paste0("phrase", x)))){
for(second_element in  eval(parse(text = paste0("number", y)))){ #format: morphology, extension, complexity
current_plus = c(paste0(first_element[1], "_plus_", second_element[1]), as.numeric2(first_element[2]) + as.numeric2(second_element[2]), as.numeric2(first_element[3]) + as.numeric2(second_element[3]))
current_minus = c(paste0(first_element[1], "_minus_", second_element[1]), as.numeric2(first_element[2]) - as.numeric2(second_element[2]), as.numeric2(first_element[3]) + as.numeric2(second_element[3]))
if(as.numeric2(current_plus[2])<100){
assign(nam2, append(eval(parse(text = nam2)), list(current_plus)))
}
if(as.numeric2(current_minus[2])> 0 & as.numeric2(current_minus[2])<100){
assign(nam2, append(eval(parse(text = nam2)), list(current_minus)))
}
}
}
}
}
}
assign(nam2, filter_within(eval(parse(text = nam2)))) #filter within
assign(nam2, filter_across(eval(parse(text = nam2)), lowest_complexity)) #filter across
lowest_complexity = append(lowest_complexity, eval(parse(text = nam2))) #append
start = start+1
}
# all = c()
#for(i in 1:depth){
# all = c(all, eval(parse(text = paste0("number", i))))
#}
#return(all)
return(lowest_complexity)
}
####################################
#Words
####################################
#possible_morphemes = c(1,2,3,4,5,6,7,8,9,10,15,20)
possible_morphemes = c(1,2)
combinations <- unlist(lapply(1:length(possible_morphemes),    # Get all combinations
combinat::combn,
x = possible_morphemes,
simplify = FALSE),
recursive = FALSE)
combinations = list(list(1), list(1,2), list(2))
####################################
# Generation of LOT expressions for the meanings (Version 1: without passing through previously defined items)
####################################
digits_classes = combinations
multiplicative_classes = combinations
#Create a data frame which will store all languages
columns= c("morphology", "extension", "complexity", "language", "word_n")
all_languages = data.frame(matrix(nrow = 0, ncol = length(columns)))
colnames(all_languages) = columns
for(dig in digits_classes){
for(multi in multiplicative_classes){
# if(length(dig)+length(multi)<16){
digits = c()#put digits in good format
for(i in dig){
digits = c(digits, list(c(as.character(i), as.character(i), 1))) # list(LoT expression, extension, complexity), for primitives complexity = 1
}
multiplicatives = c()#put multiplicatives in good format
for(i in multi){
multiplicatives = c(multiplicatives, list(c(as.character(i), as.character(i), 1))) # list(LoT expression, extension, complexity), for primitives complexity = 1
}
}}
for(dig in digits_classes){
for(multi in multiplicative_classes){
# if(length(dig)+length(multi)<16){
#digits = c()#put digits in good format
for(i in dig){
digits = c(digits, list(c(as.character(i), as.character(i), 1))) # list(LoT expression, extension, complexity), for primitives complexity = 1
}
#multiplicatives = c()#put multiplicatives in good format
for(i in multi){
multiplicatives = c(multiplicatives, list(c(as.character(i), as.character(i), 1))) # list(LoT expression, extension, complexity), for primitives complexity = 1
}
}}
digits[1]
digits[1][1]
digits[1][[1]]
k=digits[1]
k[1]
combinations = list(list(1), list(1,2), list(2))
digits_classes = combinations
multiplicative_classes = combinations
columns= c("morphology", "extension", "complexity", "language", "word_n")
all_languages = data.frame(matrix(nrow = 0, ncol = length(columns)))
colnames(all_languages) = columns
for(dig in digits_classes){
for(multi in multiplicative_classes){
# if(length(dig)+length(multi)<16){
#digits = c()#put digits in good format
for(i in dig){
digits = c(digits, list(c(as.character(i), as.character(i), 1))) # list(LoT expression, extension, complexity), for primitives complexity = 1
}
#multiplicatives = c()#put multiplicatives in good format
for(i in multi){
multiplicatives = c(multiplicatives, list(c(as.character(i), as.character(i), 1))) # list(LoT expression, extension, complexity), for primitives complexity = 1
}
}}
k=digits[1]
k[1]
setwd("~/Desktop/Numerals/Words-syntax trade-off/src")
source("./Numerals_functions.R")
options(scipen = 999)
library(minpack.lm)
####################################
# Import languages and LoT descriptions
####################################
Folder = "../data/"
for(i in 0:100){
assign(paste0("evo_full_generation_",i), read.csv(paste0("../data/evo_full_generation_",i, ".csv"), header = TRUE))
}
artificial_languages = evo_full_generation_0
for(i in 1:100){
artificial_languages = rbind(artificial_languages, eval(parse(text = paste0("evo_full_generation_",i))))
}
length(unique(artificial_languages$language))
#10572
artificial_languages = artificial_languages[c("language", "word_n", "expected_complexity")]
artificial_languages = unique(artificial_languages)
artificial_languages$type = "artificial"
natural_languages = read.csv("../data/natural_lang_complexity_measures.csv", header = TRUE)
natural_languages$type = "natural"
all_languages = rbind(artificial_languages, natural_languages)
dominant = read.csv(paste0(Folder, "pareto_dominant.csv"), header = TRUE)
dominant$type ="dominant"
#dominant = dominant %>% distinct(coordinates, .keep_all = TRUE)#keep only unique dominant points
smoothPareto <- data.frame(
with(dominant,
spline(word_n, expected_complexity, xout = seq(0.1, 30, by = 0.001))
),
method = "spline()"
)
names(smoothPareto)[names(smoothPareto) == 'x'] <- 'word_n'
names(smoothPareto)[names(smoothPareto) == 'y'] <- 'expected_complexity'
p <- ggplot(all_languages, aes(x=word_n, y=expected_complexity)) +
scale_color_manual(name="Language", values=c('#00ba92','#ff5a1d'))+
scale_shape_manual(name="Language", values=c(17, 16))+
geom_point(aes(shape=type, color=type), position = "jitter", alpha = 0.7, size = 2) +  xlab("Lexicon size (as number of lexicalized concepts)") + ylab("Average utterance length")+
xlim(0,30) + ylim(0.5,10)
#geom_point(aes(shape=type, color=type), alpha = 0.6, size = 1) +  xlab("Number of morphemes") + ylab("Expected morphosyntactic complexity")
print(p)
png("Natural-artificial-final.png", width = 140, height = 90, units='mm', res = 300)
p <- ggplot(all_languages, aes(x=word_n, y=expected_complexity)) +
scale_color_manual(name="Language", values=c('#00ba92','#ff5a1d'))+
scale_shape_manual(name="Language", values=c(17, 16))+
geom_point(aes(shape=type, color=type), position = "jitter", alpha = 0.7, size = 2) +  xlab("Lexicon size (as number of lexicalized concepts)") + ylab("Average utterance length")+
xlim(0,30) + ylim(0.5,10)
#geom_point(aes(shape=type, color=type), alpha = 0.6, size = 1) +  xlab("Number of morphemes") + ylab("Expected morphosyntactic complexity")
print(p)
dev.off()
p2 <- p +
geom_line(data = smoothPareto, size = 1)
print(p2)
png("Pareto-morphosyntax-final.png", width = 140, height = 90, units='mm', res = 300)
p2 <- p +
geom_line(data = smoothPareto, size = 1)
print(p2)
dev.off()
multiplicatives_test_languages = read.csv(paste0("../data/evo_full_generation_","the_sample", ".csv"), header = TRUE, colClasses = "character")
multiplicatives_test_languages = multiplicatives_test_languages[c("language", "word_n", "expected_complexity", "digs", "multis")]
multiplicatives_test_languages = unique(multiplicatives_test_languages)
View(multiplicatives_test_languages)
multiplicatives_test_languages$word_n = as.numeric(multiplicatives_test_languages$word_n)
multiplicatives_test_languages$expected_complexity = as.numeric(multiplicatives_test_languages$expected_complexity)
multiplicatives_test_languages$type = "artificial_first_n"
View(multiplicatives_test_languages)
all_languages_analysis2 = rbind(all_languages, multiplicatives_test_languages[, c("language", "word_n", "expected_complexity", "type")])
View(all_languages_analysis2)
levels(all_languages_analysis2$type)[levels(all_languages_analysis2$type)=="artificial"] <- "artificial_evo_alg"
View(all_languages_analysis2)
levels(all_languages_analysis2$type)[levels(all_languages_analysis2$type)=="artificial"] <- "artificial_evo_alg"
View(all_languages_analysis2)
levels(all_languages_analysis2$type)
all_languages_analysis2 = rbind(all_languages, multiplicatives_test_languages[, c("language", "word_n", "expected_complexity", "type")])
all_languages_analysis2$type = as.factor(all_languages_analysis2$type)
levels(all_languages_analysis2$type)[levels(all_languages_analysis2$type)=="artificial"] <- "artificial_evo_alg"
View(all_languages_analysis2)
p <- ggplot(all_languages_analysis2, aes(x=word_n, y=expected_complexity)) +
scale_color_manual(name="Language", values=c('#00ba92','#ff5a1d', '#0000FF'))+
scale_shape_manual(name="Language", values=c(17, 16, 15))+
geom_point(aes(shape=type, color=type), position = "jitter", alpha = 0.7, size = 1) +  xlab("Number of lexicalized concepts") + ylab("Expected morphosyntactic complexity")+
xlim(0,30) + ylim(0.5,10)
#geom_point(aes(shape=type, color=type), alpha = 0.6, size = 1) +  xlab("Number of morphemes") + ylab("Expected morphosyntactic complexity")
print(p)
p <- ggplot(all_languages_analysis2, aes(x=word_n, y=expected_complexity)) +
scale_color_manual(name="Language", values=c('#00ba92', '#0000FF', '#ff5a1d'))+
scale_shape_manual(name="Language", values=c(17, 15, 16))+
geom_point(aes(shape=type, color=type), position = "jitter", alpha = 0.7, size = 2) +  xlab("Number of lexicalized concepts") + ylab("Expected morphosyntactic complexity")+
xlim(0,30) + ylim(0.5,10)
#geom_point(aes(shape=type, color=type), alpha = 0.6, size = 1) +  xlab("Number of morphemes") + ylab("Expected morphosyntactic complexity")
print(p)
p <- ggplot(all_languages_analysis2, aes(x=word_n, y=expected_complexity)) +
scale_color_manual(name="Language", values=c('#00ba92', '#0000FF', '#ff5a1d'))+
scale_shape_manual(name="Language", values=c(17, 15, 16))+
geom_point(aes(shape=type, color=type), position = "jitter", alpha = 0.7, size = 1.5) +  xlab("Number of lexicalized concepts") + ylab("Expected morphosyntactic complexity")+
xlim(0,30) + ylim(0.5,10)
#geom_point(aes(shape=type, color=type), alpha = 0.6, size = 1) +  xlab("Number of morphemes") + ylab("Expected morphosyntactic complexity")
print(p)
p <- ggplot(all_languages, aes(x=word_n, y=expected_complexity)) +
scale_color_manual(name="Language", values=c('#00ba92','#ff5a1d'))+
scale_shape_manual(name="Language", values=c(17, 16))+
geom_point(aes(shape=type, color=type), position = "jitter", alpha = 0.7, size = 1.5) +  xlab("Lexicon size (as number of lexicalized concepts)") + ylab("Average utterance length")+
xlim(0,30) + ylim(0.5,10)
#geom_point(aes(shape=type, color=type), alpha = 0.6, size = 1) +  xlab("Number of morphemes") + ylab("Expected morphosyntactic complexity")
print(p)
png("Natural-artificial-final.png", width = 140, height = 90, units='mm', res = 300)
p <- ggplot(all_languages, aes(x=word_n, y=expected_complexity)) +
scale_color_manual(name="Language", values=c('#00ba92','#ff5a1d'))+
scale_shape_manual(name="Language", values=c(17, 16))+
geom_point(aes(shape=type, color=type), position = "jitter", alpha = 0.7, size = 1.5) +  xlab("Lexicon size (as number of lexicalized concepts)") + ylab("Average utterance length")+
xlim(0,30) + ylim(0.5,10)
#geom_point(aes(shape=type, color=type), alpha = 0.6, size = 1) +  xlab("Number of morphemes") + ylab("Expected morphosyntactic complexity")
print(p)
dev.off()
# Add Pareto front to the plot
png("Pareto-morphosyntax-final.png", width = 140, height = 90, units='mm', res = 300)
p2 <- p +
geom_line(data = smoothPareto, size = 1)
print(p2)
dev.off()
print(p2)
p2 <- p +
geom_line(data = smoothPareto, size = 0.5)
print(p2)
png("Pareto-morphosyntax-final.png", width = 140, height = 90, units='mm', res = 300)
p2 <- p +
geom_line(data = smoothPareto, size = 0.5)
print(p2)
dev.off()
png("Natural-artificial-final.png", width = 140, height = 90, units='mm', res = 300)
p <- ggplot(all_languages, aes(x=word_n, y=expected_complexity)) +
scale_color_manual(name="Language", values=c('#00ba92','#ff5a1d'))+
scale_shape_manual(name="Language", values=c(17, 16))+
geom_point(aes(shape=type, color=type), position = "jitter", alpha = 0.7, size = 1.5) +  xlab("Lexicon size (as number of lexicalized concepts)") + ylab("Average utterance length")+
xlim(0,30) + ylim(0.5,10)
#geom_point(aes(shape=type, color=type), alpha = 0.6, size = 1) +  xlab("Number of morphemes") + ylab("Expected morphosyntactic complexity")
print(p)
dev.off()
# Add Pareto front to the plot
png("Pareto-morphosyntax-final.png", width = 140, height = 90, units='mm', res = 300)
p2 <- p +
geom_line(data = smoothPareto, size = 0.5)
print(p2)
dev.off()
multiplicatives_test_languages = read.csv(paste0("../data/evo_full_generation_","the_sample", ".csv"), header = TRUE, colClasses = "character")
multiplicatives_test_languages = multiplicatives_test_languages[c("language", "word_n", "expected_complexity", "digs", "multis")]
multiplicatives_test_languages = unique(multiplicatives_test_languages)
multiplicatives_test_languages$word_n = as.numeric(multiplicatives_test_languages$word_n)
multiplicatives_test_languages$expected_complexity = as.numeric(multiplicatives_test_languages$expected_complexity)
multiplicatives_test_languages$type = "artificial_first_n"
all_languages_analysis2 = rbind(all_languages, multiplicatives_test_languages[, c("language", "word_n", "expected_complexity", "type")])
all_languages_analysis2$type = as.factor(all_languages_analysis2$type)
levels(all_languages_analysis2$type)[levels(all_languages_analysis2$type)=="artificial"] <- "artificial_evo_alg"
p <- ggplot(all_languages_analysis2, aes(x=word_n, y=expected_complexity)) +
scale_color_manual(name="Language", values=c('#00ba92', '#0000FF', '#ff5a1d'))+
scale_shape_manual(name="Language", values=c(17, 15, 16))+
geom_point(aes(shape=type, color=type), position = "jitter", alpha = 0.7, size = 1.5) +  xlab("Number of lexicalized concepts") + ylab("Expected morphosyntactic complexity")+
xlim(0,30) + ylim(0.5,10)
#geom_point(aes(shape=type, color=type), alpha = 0.6, size = 1) +  xlab("Number of morphemes") + ylab("Expected morphosyntactic complexity")
print(p)
png("Pareto-morphosyntax-multiplicatives.png", width = 140, height = 90, units='mm', res = 300)
p2 <- p +
geom_line(data = smoothPareto, size = 0.5)
print(p2)
dev.off()
